# Local Llama Configuration
MODEL_PATH=./models/llama-2-7b-chat.q4_0.bin

# Generation Settings
MAX_TOKENS=512
TEMPERATURE=0.7

# Hardware Settings
GPU_LAYERS=0
N_THREADS=4
N_CTX=2048
